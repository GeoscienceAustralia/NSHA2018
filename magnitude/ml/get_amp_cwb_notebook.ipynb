{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import local_magnitude\n",
    "from NSHA2018.magnitude.mw.Main.scripts import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/opt/anaconda-2.3.0/bin/python2\n",
    "\"\"\"#!/opt/antelope/5.4/bin/python\"\"\"\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from obspy.core import utcdatetime, event\n",
    "from obspy.core.event import Catalog, Event, Magnitude, Origin, StationMagnitude\n",
    "#from obspy.neic.client import Client\n",
    "from obspy.clients.neic.client import Client\n",
    "from obspy.io.xseed import Parser\n",
    "#from obspy.core.util import gps2DistAzimuth\n",
    "from obspy.geodetics import gps2dist_azimuth as gps2DistAzimuth\n",
    "#from obspy.taup import TauPyModel\n",
    "from obspy.signal import invsim as inv\n",
    "from obspy.io.xseed.utils import SEEDParserException\n",
    "# plot all traces into one pdf file\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.dates import date2num\n",
    "# we will use dataless seed from IRIS to get station information\n",
    "parser = Parser(\"../../../AU.dataless\")\n",
    "# travel-time model will be iasp91 but could be any\n",
    "from obspy import taup\n",
    "vel_model = taup.TauPyModel(model=\"iasp91\")\n",
    "#from obspy.taup.TauPyModel import get_travel_times\n",
    "# local modules\n",
    "from NSHA2018.magnitude.ml import local_magnitude\n",
    "r_earth = 6371\n",
    "def sind(x): return np.sin(x / 180. * np.pi)\n",
    "def cosd(x): return np.cos(x / 180. * np.pi)\n",
    "def tand(x): return np.tan(x / 180. * np.pi)\n",
    "def arcsind(x): return np.arcsin(x) / np.pi * 180\n",
    "def arccosd(x): return np.arccos(x) / np.pi * 180\n",
    "def arctand(x): return np.arctan(x) / np.pi * 180\n",
    "def gps2DistDegree(lat1, lon1, lat2, lon2):\n",
    "    return arccosd(sind(lat1) * sind(lat2) +\n",
    "                   cosd(lat1) * cosd(lat2) * cosd(lon1 - lon2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we define how we measure peak to peak amplitude and period\n",
    "def max_p2t(data, delta):\n",
    "     \"\"\"\n",
    "     Function to find the maximum peak-to-trough amplitude and period of this \\\n",
    "     amplitude.\n",
    "\n",
    "     :type data: ndarray\n",
    "     :param data: waveform trace to find the peak-to-trough in.\n",
    "     :type delta: float\n",
    "     :param delta: Sampling interval in seconds\n",
    "\n",
    "     :returns: tuple of (amplitude, period, time) with amplitude in the same \\\n",
    "         scale as given in the input data, and period in seconds, and time in \\\n",
    "         seconds from the start of the data window.\n",
    "     \"\"\"\n",
    "     turning_points = []  # A list of tuples of (amplitude, sample)\n",
    "     for i in range(1, len(data) - 1):\n",
    "         if (data[i] < data[i-1] and data[i] < data[i+1]) or\\\n",
    "            (data[i] > data[i-1] and data[i] > data[i+1]):\n",
    "             turning_points.append((data[i], i))\n",
    "     if len(turning_points) >= 1:\n",
    "         amplitudes = np.empty([len(turning_points)-1],)\n",
    "         half_periods = np.empty([len(turning_points)-1],)\n",
    "     else:\n",
    "         print('Turning points has length: '+str(len(turning_points)) +\n",
    "               ' data have length: '+str(len(data)))\n",
    "         return (0.0, 0.0, 0.0)\n",
    "     for i in range(1, len(turning_points)):\n",
    "         half_periods[i-1] = (delta * (turning_points[i][1] -\n",
    "                                       turning_points[i-1][1]))\n",
    "         amplitudes[i-1] = np.abs(turning_points[i][0]-turning_points[i-1][0])\n",
    "     amplitude = np.max(amplitudes)\n",
    "     period = 2 * half_periods[np.argmax(amplitudes)]\n",
    "\n",
    "     return (amplitude, period, delta*turning_points[np.argmax(amplitudes)][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the cwb port\n",
    "client=Client(host='10.7.161.60',port=2061,debug=False, nonice=True)\n",
    "eq=[]\n",
    "# here we read all events line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate catalogue object\n",
    "catalogue = Catalog()\n",
    "# Build Quakeml Event object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233 Trace(s) in Stream:\n",
      "\n",
      "AU.KDU..BHE | 2014-01-01T08:32:15.025000Z - 2014-01-01T08:52:15.025000Z | 40.0 Hz, 48001 samples\n",
      "...\n",
      "(231 other traces)\n",
      "...\n",
      "AU.QLP..BHE | 2014-01-01T08:32:15.025000Z - 2014-01-01T08:52:15.025000Z | 40.0 Hz, 48001 samples\n",
      "\n",
      "[Use \"print(Stream.__str__(extended=True))\" to print all Traces]\n",
      "ADE,,local,None,2014,1,1,08,32,15.2,ACDT,10.5,138.813,-31.914,10,?,ML,2.5,,,,,47,0.59,,,,,\"E of Hawker, SA\",2014-01-01  0832,20140101083215*,2014.0009746 ,36,E,Hawker,SA\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/users/u61092/unix/.local/lib/python2.7/site-packages/obspy/io/mseed/core.py:384: InternalMSEEDReadingWarning: readMSEEDBuffer(): Unknown error '105' in record starting at offset 593408. The rest of the file will not be read.\n",
      "  warnings.warn(*_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Trace(s) in Stream:\n",
      "\n",
      "AU.KDU..BHE | 2014-01-01T11:55:55.025000Z - 2014-01-01T12:15:55.025000Z | 40.0 Hz, 48001 samples\n",
      "...\n",
      "(89 other traces)\n",
      "...\n",
      "AU.ARPS..SHN | 2014-01-01T11:55:55.025000Z - 2014-01-01T12:15:55.025000Z | 40.0 Hz, 48001 samples\n",
      "\n",
      "[Use \"print(Stream.__str__(extended=True))\" to print all Traces]\n",
      "AUST,,local,None,2014,1,1,11,55,55.35,ACST,9.5,133.867,-19.706,10,,ML,2.5,0.52,13.2,4,26,6,0.76,162.8,,0.51,7.95,\"WSW of Tennant Creek, NT\",2014-01-01  1155,20140101115555*,2014.0013621 ,35,WSW,Tennant Creek,NT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# file to store outputs\n",
    "event_num = 0\n",
    "f_out = open('amp_cat.txt', 'w')\n",
    "with open(\"../../../GGCat_2014.csv\",'r') as cat:\n",
    "    for line in cat:\n",
    "        \n",
    "        eq=line.split(',')\n",
    "        yr=int(eq[4])\n",
    "        mon=int(eq[5])\n",
    "        day=int(eq[6])\n",
    "        hr=int(eq[7])\n",
    "        mn=int(eq[8])\n",
    "        sec=float(eq[9])\n",
    "        code=eq[10]\n",
    "        corr=float(eq[11])\n",
    "        lon=float(eq[12])\n",
    "        lat=float(eq[13]) #Latitudes in old Gary Gibson catalogue given in southern hemisphere coordindates\n",
    "        dep=float(eq[14])\n",
    "        mag_type = eq[16]\n",
    "        mag_pref = float(eq[17])\n",
    "        mag_source = eq[31].split(' ')[-1]\n",
    "\n",
    "        start_time=utcdatetime.UTCDateTime(yr,mon,day,hr,mn,int(sec),int((sec-int(sec))*100000))\n",
    "#        ''' correct the time if not UTC '''\n",
    "# Is this really needed??? Looks like may already be converted??\n",
    "#        if code==\"AEST\" :\n",
    "#            start_time-=36000\n",
    "#        if code==\"WITA\" or code==\"AWST\":\n",
    "#            start_time-=28800\n",
    "        \n",
    "        # Build event object\n",
    "        event = Event(resource_id='GG_cat_' + str(event_num), creation_info='JG')\n",
    "        event_num += 1\n",
    "        origin = Origin()\n",
    "        origin.time = start_time\n",
    "        origin.longitude = lon\n",
    "        origin.latitude = lat\n",
    "        origin.depth = dep\n",
    "        event.origins.append(origin)\n",
    "        mag = Magnitude(creation_info='GG_cat')\n",
    "        mag.mag = mag_pref\n",
    "        mag.magnitude_type = mag_type\n",
    "        event.magnitudes.append(mag)\n",
    "        \n",
    "\n",
    "        ''' the time window to request the data will be 20 minutes, check maximum travel time and increase this value accordingly '''\n",
    "        end_time=start_time+1200 # 20 minutes - to catch ~Rayleigh waves at 20 degrees distance (assume v>=3km/s)\n",
    "        ''' get all waveform data available, use wildcards to reduce the data volume and speedup the process,\n",
    "        unfortunately we need to request few times for every number of characters that forms the station name '''\n",
    "        st_3 = client.get_waveforms(\"AU\", \"???\", \"\", \"[BS]?[ENZ]\", start_time,end_time)\n",
    "        st_4 = client.get_waveforms(\"AU\", \"????\", \"\", \"[BS]?[ENZ]\", start_time,end_time)\n",
    "        if len(st_4) > 0:\n",
    "            st=st_3+st_4\n",
    "        else:\n",
    "            st=st_3\n",
    "        print st\n",
    "        # Cleanup duplicate traces returned by server\n",
    " #       st.merge(-1) #-1 method only merges overlapping or adjacent traces with same i            \n",
    "        # Now sort the streams by station and channel\n",
    "        st.sort()\n",
    "        # Cleanup duplicate traces returned by server\n",
    "        st.merge(1, fill_value=None) #1 method only merges overlapping or adjacent traces with same id\n",
    "        # Now sort the streams by station and channel\n",
    "        st.sort()\n",
    "        \n",
    "        # Filter the stream - should it be done here on by trace???\n",
    "        st.filter('bandpass', freqmin=0.5, freqmax=10., corners=6)#, df=sample_rate)\n",
    "        # Demean\n",
    "        st.detrend('demean')\n",
    "  #      st.filter(bandpass, freqmin=0.5, freqmax=10., )\n",
    "        # PDF file to plot all traces for this event\n",
    "##        figpdf = PdfPages('test.pdf')\n",
    "        try:\n",
    "            st_figure = st.plot(equal_scale=False, handle=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        ''' first we will print the record of the earthquake as in catalogue '''\n",
    "        print line\n",
    "        \n",
    "        counter = 0 # For debugging only\n",
    "        mag_list = []\n",
    "        distance_list = []\n",
    "        for tr in st:\n",
    "            counter +=1\n",
    "            ''' get station ID'''\n",
    "            seedid=tr.get_id()\n",
    "            channel = tr.stats.channel\n",
    "            # Filter to only get stations within 20 degrees\n",
    "            try:\n",
    "                ''' get station coordinates from dataless seed '''\n",
    "                tr.stats.coordinates = parser.get_coordinates(seedid,start_time)\n",
    "                tr.stats.distance = gps2DistDegree(tr.stats.coordinates.latitude,tr.stats.coordinates.longitude,lat,lon)            \n",
    "            except (SEEDParserException,AssertionError):\n",
    "         #       print  tr.stats['station'],tr.stats['channel'],'-1,-1,-1'\n",
    "                continue\n",
    "            if tr.stats.distance > 20: # only use stations within 20 degrees\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "\n",
    "#            numobs = tr.count()\n",
    "#            print 'count', numobs\n",
    "#            print 'seedid', seedid\n",
    "#            print 'sample rate', tr.stats.sampling_rate\n",
    "            sample_rate = tr.stats.sampling_rate\n",
    "\n",
    "\n",
    "            try:\n",
    "                tr.stats.great_circle_distance, azf, azb = \\\n",
    "                    gps2DistAzimuth(tr.stats.coordinates.latitude,tr.stats.coordinates.longitude,lat,lon)\n",
    "                travel_times=vel_model.get_travel_times(dep, tr.stats.distance, \\\n",
    "                                                        phase_list = ['P', 'p', 'Pn', 'S', 'Sn'])#,model=\"iasp91\") # need to make more efficient in order\n",
    "                # to only calculate once per station\n",
    "                #print travel_times, type(travel_times)\n",
    "                #try:\n",
    "                    #for arrival in travel_times:\n",
    "                    #    print arrival.phase.name\n",
    "                #   arrivals= (item for item in travel_times if item.phase.name=='S' or item.phase.name=='Sn').next()\n",
    "                #except StopIteration:\n",
    "                #   print \"WARNING: reference station \",tr.stats['station'],\" does not have S or Sn phases\"\n",
    "                P_time = None\n",
    "                S_time = None\n",
    "                for arrival in travel_times:\n",
    "                 #   print arrival.phase.name\n",
    "                    if arrival.phase.name == 'P' or arrival.phase.name == 'p' or arrival.phase.name == 'Pn':\n",
    "                        if P_time is not None:\n",
    "                            P_time = min(P_time, arrival.time)\n",
    "                        else:\n",
    "                            P_time = arrival.time\n",
    "                    if arrival.phase.name == 'S' or arrival.phase.name == 'Sn':\n",
    "                        if S_time is not None:\n",
    "                            S_time = min(S_time, arrival.time)\n",
    "                        else:\n",
    "                            S_time = arrival.time\n",
    "#                print 'P_time', P_time\n",
    "#                print 'S_time', S_time\n",
    "                # Convert travel times to days and add start-time\n",
    "                # See https://github.com/obspy/obspy/blob/master/obspy/imaging/waveform.py line 694\n",
    "                # Then plot over waveforms\n",
    "                SECONDS_PER_DAY = 3600.0 * 24.0\n",
    "                try:\n",
    "                    p_value = ((P_time / SECONDS_PER_DAY) +\n",
    "                                date2num(tr.stats.starttime.datetime))\n",
    "                except ValueError:\n",
    "                    p_value = None\n",
    "                try:\n",
    "                    s_value = ((S_time / SECONDS_PER_DAY) +\n",
    "                                date2num(tr.stats.starttime.datetime))\n",
    "                except:\n",
    "                    s_value = None\n",
    "                    \n",
    "                try:\n",
    "                    st_figure.axes[counter-1].axvline(p_value, 0, 1, c='r')\n",
    "                    st_figure.axes[counter-1].axvline(s_value, 0, 1, c='b')\n",
    "                except:\n",
    "                    pass\n",
    "                # Save figures\n",
    "             #   fig_name = \n",
    "               # pyplot.savefig()\n",
    "                # Signal to noise tests\n",
    "                try:\n",
    "                    #sn = modules.sn_test(tr, start_time+P_time, 3*60., 0.36*tr.stats.great_circle_distance/1000. + 60, 20.)\n",
    "                    sn, m1, m2 = modules.sn_test(tr, start_time+P_time, 30., 60, 5.)#3*(S_time-P_time), 5.)\n",
    "                    snr = modules.snr_test(tr, start_time+P_time, 30., 60, [0.5, 10], 0.75)\n",
    "              #  print 'sn', sn, m1, m2\n",
    "              #  print snr\n",
    "                except:\n",
    "                    print 'Could not do signal to noise test'\n",
    "                    sn = 0\n",
    "                    snr = 0\n",
    "               # snr = modules.snr_test()\n",
    "#                print arrivals\n",
    "#                for key, value in arrivals.iteritems():\n",
    "#                        print key, value\n",
    "##                S_time = start_time + arrivals['time']\n",
    "##                figure = tr.plot(handle=True)\n",
    "                # Add S arrival time\n",
    "##                pyplot.scatter([S_time],[0], marker = 'o', s=40, c='r')\n",
    "##                try:\n",
    "##                   figpdf.savefig(figure)\n",
    "##                    figure.close()\n",
    "##               except:\n",
    "##                    pass\n",
    "               # figpdf.close()\n",
    "#                  print travel_times\n",
    "                ''' lets select 20 seconds window from S-wave p2p measurements '''\n",
    "#               print start_time+arrivals['time'],start_time+arrivals['time']+20,end_time\n",
    " #               wave=tr.slice(start_time+arrivals['time'],start_time+arrivals['time']+20)\n",
    " #               if len(wave.data)>0:\n",
    " #                  amplitude,period,delay=max_p2t(wave.data,wave.stats.delta)\n",
    " #               else:\n",
    " #                   amplitude,period,delay = None,None,None\n",
    " #               if amplitude > 0. and period > 0. :\n",
    "                paz=parser.get_paz(seedid,start_time)\n",
    "                ''' calib is already applied therefore we set sensitivity to 1 '''\n",
    "                paz['sensitivity']=1.\n",
    "                # Define Wood Anderson paz based on Uhrhammer 1990 sensitivity of 2080\n",
    "                # rather than theoretical 2800 as used by obpy\n",
    "                # wa_amp=inv.estimateWoodAndersonAmplitude(paz,amplitude,period)\n",
    "                paz_wa = {'sensitivity': 2080, 'zeros': [0j], 'gain': 1,\n",
    "                          'poles': [-6.2832 - 4.7124j, -6.2832 + 4.7124j]}\n",
    "                # Now convert to WA spectra\n",
    "                wa_tr = tr.simulate(paz_remove=paz, paz_simulate=paz_wa)\n",
    "                # Repeat signal to noise tests on displacement\n",
    "                try:\n",
    "                    sn_disp, m1, m2 = modules.sn_test(wa_tr, start_time+P_time, 30., 60., 5.)\n",
    "                except:\n",
    "                    sn_disp = 0\n",
    "       #         print 'sn_dip', sn_disp, m1, m2\n",
    "                # Plot WA record\n",
    "##                figure = wa_tr.plot(handle=True)\n",
    "##                try:\n",
    "##                    figpdf.savefig(figure)\n",
    "##                    figure.close()\n",
    "##                except:\n",
    "##                    pass\n",
    "                # Calculate WA amplitudes and periods\n",
    "#                wave=wa_tr.slice(start_time+arrivals['time'],start_time+arrivals['time']+20)\n",
    "#                print len(wave.data)\n",
    "                if len(wa_tr.data)>0:\n",
    "                    wa_amp,period,delay=max_p2t(wa_tr.data,wa_tr.stats.delta)\n",
    "                    local_mag = local_magnitude.calculate_local_magnitude(wa_amp/10e6, \\\n",
    "                                                                          [lon,lat,dep],\\\n",
    "                                                                          tr.stats.great_circle_distance/1000.)\n",
    "                                        \n",
    "                else:\n",
    "                    wa_amp,period,delay = None,None,None\n",
    "                    local_mag = None\n",
    "                ''' now we print every station-component measurement '''\n",
    "                #local_magnitude.calculate_local_magnitude\n",
    "                \n",
    "     #           print tr.stats['station'],tr.stats['channel'],tr.stats['calib'], \\\n",
    "      #              tr.stats.great_circle_distance/1000, wa_amp/10e6,period, local_mag\n",
    "                mag_list.append(local_mag)\n",
    "                distance_list.append(tr.stats.great_circle_distance/1000)\n",
    "                outlist = [tr.stats['station'],tr.stats['channel'],tr.stats['calib'], \\\n",
    "                           tr.stats.coordinates.latitude,tr.stats.coordinates.longitude, \\\n",
    "                           lat, lon, dep, wa_amp, period, tr.stats.great_circle_distance, \\\n",
    "                           sn, snr, sn_disp]\n",
    "                for item in outlist:\n",
    " #                   print item\n",
    "                    f_out.write(\"%s,\" % item)\n",
    "                f_out.write('\\n')\n",
    "                #else:\n",
    "                #    print tr.stats['station'],tr.stats['channel'],tr.stats['calib'],amplitude,period\n",
    "            except (SEEDParserException,AssertionError):\n",
    "                pass\n",
    "                #print  tr.stats['station'],tr.stats['channel'],'-1,-1,-1'\n",
    "##            if counter > 10:\n",
    "##                break\n",
    "##    figpdf.close()\n",
    "f_out.close()\n",
    "#mean_mag = np.mean(mag_list)\n",
    "##print 'mean ml', mean_mag\n",
    "#pyplot.scatter(distance_list, mag_list)\n",
    "#pyplot.plot([0,np.max(distance_list)], [mean_mag,mean_mag])\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
